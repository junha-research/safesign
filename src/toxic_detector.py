# src/toxic_detector.py
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from .llm_service import get_genai_client
from deepeval.models.base_model import DeepEvalBaseLLM

# [ë³€ê²½] ê¸°ì¡´ rag_pipeline ëŒ€ì‹  ìƒˆë¡œ ë§Œë“  Context Manager ì‚¬ìš©
from .legal_context import LawContextManager 
from .rag_pipeline import search_precedents # íŒë¡€ëŠ” ê¸°ì¡´ëŒ€ë¡œ ìœ ì§€ (ì´ë¯¸ DBê°€ ìˆìœ¼ë¯€ë¡œ)
from dotenv import load_dotenv
from deepeval.metrics.g_eval import Rubric
load_dotenv()

# --- 1. DeepEvalìš© Gemini Wrapper ì„¤ì • ---
class GeminiDeepEvalLLM(DeepEvalBaseLLM):
    def __init__(self, model_name="gemini-2.5-flash"):
        self.client = get_genai_client()
        self.model_name = model_name

    def load_model(self):
        return self.client

    def generate(self, prompt: str) -> str:
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            return response.text
        except Exception as e:
            return f"Error: {e}"

    async def a_generate(self, prompt: str) -> str:
        # ë¹„ë™ê¸° ì²˜ë¦¬ê°€ í•„ìš”í•  ê²½ìš° êµ¬í˜„ (ì—¬ê¸°ì„  ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ)
        return self.generate(prompt)

    def get_model_name(self):
        return self.model_name

# --- 2. ë…ì†Œì¡°í•­ íŒë³„ê¸° í´ë˜ìŠ¤ ---
class ToxicClauseDetector:
    def __init__(self):
        self.evaluator_llm = GeminiDeepEvalLLM()
        # [ì¶”ê°€] ë²•ë ¹ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.law_manager = LawContextManager()
        self.law_manager.initialize_database() # ê°ì²´ ìƒì„± ì‹œ 1ë²ˆë§Œ ì‹¤í–‰ë¨ (ì•½ 3~5ì´ˆ ì†Œìš”)
        # [í•µì‹¬] G-Eval í‰ê°€ ê¸°ì¤€ (Rubric) ì •ì˜
        self.toxic_criteria = """
        ë‹¹ì‹ ì€ í•œêµ­ì˜ ê·¼ë¡œê¸°ì¤€ë²•ì„ ìˆ˜í˜¸í•˜ëŠ” ì—„ê²©í•œ 'ê·¼ë¡œê³„ì•½ì„œ ê°ì‚¬ê´€'ì…ë‹ˆë‹¤.
        ì…ë ¥ëœ 'ê·¼ë¡œê³„ì•½ ì¡°í•­'ì´ 'ê´€ë ¨ ë²•ë ¹/íŒë¡€(Context)'ì— ë¹„ì¶”ì–´ ë³¼ ë•Œ ê·¼ë¡œìì—ê²Œ ë¶ˆë¦¬í•˜ê±°ë‚˜ ë¶ˆë²•ì ì¸ì§€ í‰ê°€í•˜ì„¸ìš”.
        
        [ë…ì†Œì¡°í•­ íŒë‹¨ ê¸°ì¤€]
        1. ê°•í–‰ê·œì • ìœ„ë°˜: ìµœì €ì„ê¸ˆ ë¯¸ë‹¬, í‡´ì§ê¸ˆ í¬ê¸° ê°ì„œ, ìœ„ì•½ê¸ˆ ì˜ˆì • ë“± ë²•ìœ¼ë¡œ ê¸ˆì§€ëœ ë‚´ìš©ì¸ê°€?
        2. í¬ê´„ì„ê¸ˆ ì˜¤ë‚¨ìš©: ê·¼ë¡œì‹œê°„ ì‚°ì •ì´ ê°€ëŠ¥í•œë°ë„ í¬ê´„ì„ê¸ˆì œë¥¼ ì ìš©í•˜ì—¬ ìˆ˜ë‹¹ì„ ë¯¸ì§€ê¸‰í•˜ë ¤ í•˜ëŠ”ê°€?
        3. ë¶ˆê³µì •ì„±: 'ê°‘'ì—ê²Œ ì¼ë°©ì ìœ¼ë¡œ ìœ ë¦¬í•˜ê±°ë‚˜, ëª¨í˜¸í•œ í‘œí˜„ìœ¼ë¡œ 'ì„'ì˜ ê¶Œë¦¬ë¥¼ ì œí•œí•˜ëŠ”ê°€?
        4. ì ˆì°¨ ë¬´ì‹œ: í•´ê³ , ì§•ê³„ ë“±ì˜ ì ˆì°¨ë¥¼ ë²•ì  ê¸°ì¤€ë³´ë‹¤ ê°„ì†Œí™”í•˜ê±°ë‚˜ ìƒëµí•˜ëŠ”ê°€?
        """
        self.rubric = [
        Rubric(score_range=(0,2), expected_outcome="ì™„ì „íˆ ë¶ˆê³µì •í•œ ê³„ì•½."),
        Rubric(score_range=(3,5), expected_outcome="ëŒ€ì²´ë¡œ ë¶ˆê³µì •í•œ ê³„ì•½."),
        Rubric(score_range=(6,7), expected_outcome="ê³µì •í•œ ê³„ì•½ì´ì§€ë§Œ ì ì¬ì ìœ¼ë¡œ ë¶ˆê³µì •í•  ìˆ˜ ìˆëŠ” ì¡°í•­ ì¡´ì¬."),
        Rubric(score_range=(8,10), expected_outcome="ë¶ˆê³µì •í•œ ì¡°í•­ ì—†ì´ ì™„ì „íˆ ê³µì •í•œ ê³„ì•½"),
    ]
        self.evaluation_steps = [
            "ì…ë ¥ëœ 'ê³„ì•½ ì¡°í•­'ì˜ í•µì‹¬ ì£¼ì œ(ì„ê¸ˆ, ê·¼ë¡œì‹œê°„, í•´ê³  ë“±)ë¥¼ íŒŒì•…í•œë‹¤.",
            "ì œê³µëœ 'Context(ë²•ë ¹/íŒë¡€)'ì™€ ì¡°í•­ì„ ë¹„êµí•˜ì—¬ ë²•ì  ê¸°ì¤€(Minimum Standard)ë³´ë‹¤ ë‚®ì€ì§€ í™•ì¸í•œë‹¤.",
            "ì¡°í•­ì— 'ìœ„ì•½ê¸ˆ', 'ì†í•´ë°°ìƒì•¡ ì˜ˆì •', 'ë¯¼í˜•ì‚¬ìƒ ì´ì˜ ì œê¸° ê¸ˆì§€' ë“±ì˜ ë¶ˆë²• í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤.",
            "ìœ„ë°˜ ì‚¬í•­ì´ ë°œê²¬ë˜ë©´ 'ë…ì†Œì¡°í•­(Toxic)'ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë†’ì€ ìœ„í—˜ ì ìˆ˜ë¥¼ ë¶€ì—¬í•œë‹¤.",
            "ë²•ì  ë¬¸ì œê°€ ì—†ë‹¤ë©´ ë‚®ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•œë‹¤."
        ]

    def _retrieve_context(self, clause_text):
        """
        Context ê²€ìƒ‰ ë¡œì§ ë³€ê²½ (API í˜¸ì¶œ -> ë¡œì»¬ DB ê²€ìƒ‰)
        """
        # 1. ë²•ë ¹ ê²€ìƒ‰ (ì´ì œ ì—„ì²­ ë¹ ë¦„!)
        laws = self.law_manager.search_relevant_laws(clause_text, k=2)
        law_text = "\n".join(laws) if laws else "ê´€ë ¨ ë²•ë ¹ ì—†ìŒ"

        # 2. íŒë¡€ ê²€ìƒ‰
        precedents = search_precedents(clause_text, k=1)
        precedent_text = precedents[0] if precedents else "ê´€ë ¨ íŒë¡€ ì—†ìŒ"

        return f"[ê´€ë ¨ ë²•ë ¹]\n{law_text}\n\n[ê´€ë ¨ íŒë¡€]\n{precedent_text}"

    def detect(self, clause_text):
        """
        ë‹¨ì¼ ì¡°í•­ì„ ë¶„ì„í•˜ì—¬ ë…ì†Œì¡°í•­ ì—¬ë¶€, ì ìˆ˜, ê·¼ê±°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        print(f"ğŸ•µï¸ ì¡°í•­ ë¶„ì„ ì¤‘: {clause_text[:30]}...")
        
        # 1. Retrieval
        retrieved_context = self._retrieve_context(clause_text)
        
        # 2. G-Eval Metric ì„¤ì •
        toxic_metric = GEval(
            name="Toxic Clause Score",
            criteria=self.toxic_criteria,
            rubric=self.rubric,
            # evaluation_steps=self.evaluation_steps,
            model=self.evaluator_llm,
            threshold=0.5, # 0.5 ì´ìƒì´ë©´ ë…ì†Œì¡°í•­ìœ¼ë¡œ ê°„ì£¼ (ì„¤ì •ì— ë”°ë¼ ë‹¤ë¦„)
            evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
        )

        # 3. Test Case ìƒì„±
        test_case = LLMTestCase(
            input=clause_text,
            actual_output="ì´ ì¡°í•­ì€ í‰ê°€ ëŒ€ìƒì…ë‹ˆë‹¤.", # G-Evalì€ Outputì´ ì—†ì–´ë„ Input-Context ê´€ê³„ í‰ê°€ ê°€ëŠ¥
            retrieval_context=[retrieved_context]
        )

        # 4. í‰ê°€ ì‹¤í–‰
        toxic_metric.measure(test_case)
        
        # 5. ê²°ê³¼ í¬ë§·íŒ…
        # G-Eval ì ìˆ˜ëŠ” 0~1 ì‚¬ì´ë¡œ ë‚˜ì˜µë‹ˆë‹¤. (0: ì•ˆì „, 1: ë§¤ìš° ìœ„í—˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ì¡°ì • í•„ìš”)
        # DeepEval ê¸°ë³¸ì€ 'ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê²ƒ'ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, 
        # ì ìˆ˜ê°€ ë†’ê²Œ ë‚˜ì™”ë‹¤ë©´ "ë²•ì ìœ¼ë¡œ ì™„ë²½í•¨", ë‚®ë‹¤ë©´ "ìœ„ë°˜ë¨"ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # **ì£¼ì˜**: í”„ë¡¬í”„íŠ¸ì—ì„œ "ìœ„ë°˜ë˜ë©´ ë†’ì€ ì ìˆ˜"ë¼ê³  ëª…ì‹œí•˜ê±°ë‚˜, í•´ì„ì„ ë°˜ëŒ€ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.
        # ì—¬ê¸°ì„œëŠ” scoreë¥¼ "ì•ˆì „ë„(Safety Score)"ë¡œ í•´ì„í•˜ê² ìŠµë‹ˆë‹¤. (ì ìˆ˜ê°€ ë‚®ìœ¼ë©´ ìœ„í—˜)
        
        safety_score = toxic_metric.score # 0.0 ~ 1.0
        risk_score = 1.0 - safety_score # ìœ„í—˜ë„ë¡œ ë³€í™˜ (0: ì•ˆì „, 1: ìœ„í—˜)
        
        is_toxic = risk_score > 0.4 # ìœ„í—˜ë„ 0.4 ì´ˆê³¼ì‹œ ë…ì†Œì¡°í•­ íŒë‹¨
        
        return {
            "clause": clause_text,
            "is_toxic": is_toxic,
            "risk_score": round(risk_score * 10, 1), # 10ì  ë§Œì  í™˜ì‚°
            "reason": toxic_metric.reason,
            "context_used": retrieved_context
        }

    def generate_easy_suggestion(self, detection_result):
        """
        íŒë³„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'ì‰¬ìš´ í•´ì„'ê³¼ 'ìˆ˜ì • ì œì•ˆ'ì„ ìƒì„±í•©ë‹ˆë‹¤. (Generator)
        """
        if not detection_result['is_toxic']:
            return "âœ… ë²•ì ìœ¼ë¡œ ë¬¸ì œì—†ëŠ” ì•ˆì „í•œ ì¡°í•­ì…ë‹ˆë‹¤."

        prompt = f"""
        ë‹¹ì‹ ì€ ê·¼ë¡œì í¸ì¸ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì¡°í•­ì´ 'ë…ì†Œì¡°í•­'ìœ¼ë¡œ íŒë³„ë˜ì—ˆìŠµë‹ˆë‹¤.
        
        [ì›ë¬¸ ì¡°í•­]: {detection_result['clause']}
        [ìœ„í—˜ íŒë‹¨ ê·¼ê±°]: {detection_result['reason']}
        [ì°¸ê³  ë²•ë ¹/íŒë¡€]: {detection_result['context_used']}

        ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. **ì‰¬ìš´ í•´ì„**: ì´ ì¡°í•­ì´ ì™œ ìœ„í—˜í•œì§€ ì´ˆë“±í•™ìƒë„ ì•Œê¸° ì‰½ê²Œ ì„¤ëª… (2ë¬¸ì¥ ì´ë‚´)
        2. **ìˆ˜ì • ì œì•ˆ**: ê·¼ë¡œìì—ê²Œ ìœ ë¦¬í•˜ê±°ë‚˜ ë²•ì— ë§ê²Œ ìˆ˜ì •í•œ ì¡°í•­ ì˜ˆì‹œ
        """
        
        return self.evaluator_llm.generate(prompt)

# --- ì‹¤í–‰ í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    detector = ToxicClauseDetector()
    
    # í…ŒìŠ¤íŠ¸ìš© ë…ì†Œì¡°í•­ ì˜ˆì‹œ
    toxic_clause = "ì œ10ì¡° (í‡´ì§ê¸ˆ) ê·¼ë¡œìê°€ ì…ì‚¬ í›„ 1ë…„ ì´ë‚´ì— í‡´ì‚¬í•˜ëŠ” ê²½ìš°, íšŒì‚¬ëŠ” êµìœ¡ë¹„ ëª…ëª©ìœ¼ë¡œ í‡´ì§ê¸ˆì„ ì§€ê¸‰í•˜ì§€ ì•„ë‹ˆí•˜ë©°, ê·¼ë¡œìëŠ” ì´ì— ëŒ€í•´ ë¯¼í˜•ì‚¬ìƒ ì´ì˜ë¥¼ ì œê¸°í•  ìˆ˜ ì—†ë‹¤."
    
    print("\nğŸš€ [ë…ì†Œì¡°í•­ íŒë³„ ì‹œì‘]")
    result = detector.detect(toxic_clause)
    
    print(f"\nğŸ“Š ìœ„í—˜ë„: {result['risk_score']} / 10")
    print(f"ğŸš¨ ë…ì†Œì¡°í•­ ì—¬ë¶€: {'ë„¤, ìœ„í—˜í•©ë‹ˆë‹¤!' if result['is_toxic'] else 'ì•„ë‹ˆì˜¤, ì•ˆì „í•©ë‹ˆë‹¤.'}")
    print(f"ğŸ“ íŒë‹¨ ê·¼ê±°: {result['reason']}")
    
    print("\nğŸ’¡ [AI ì†”ë£¨ì…˜]")
    suggestion = detector.generate_easy_suggestion(result)
    print(suggestion)