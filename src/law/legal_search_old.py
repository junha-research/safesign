import os
import requests
import xml.etree.ElementTree as ET
import json  # JSON ì²˜ë¦¬ë¥¼ ìœ„í•´ import ì¶”ê°€
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.documents import Document

# 1. í™˜ê²½ ì„¤ì •
load_dotenv()
MOLEG_API_KEY = os.getenv("MOLEG_API_KEY")
SAVE_PATH = "../data/faiss_law_db"
TARGET_LAWS = ["ê·¼ë¡œê¸°ì¤€ë²•", "ìµœì €ì„ê¸ˆë²•", "ê·¼ë¡œìí‡´ì§ê¸‰ì—¬ ë³´ì¥ë²•"]

# 2. í—¬í¼ í•¨ìˆ˜ ì •ì˜
def search_law_id(law_name):
    """ë²•ë ¹ ì´ë¦„ìœ¼ë¡œ ID ê²€ìƒ‰ (JSON ì‘ë‹µ íŒŒì‹±)"""
    url = f"http://www.law.go.kr/DRF/lawSearch.do?OC={MOLEG_API_KEY}&target=eflaw&nw=3&query={law_name}&type=json"
    try:
        response = requests.get(url, timeout=5)
        data = response.json()
        laws = data.get("LawSearch", {}).get("law", [])
        if isinstance(laws, dict):
            laws = [laws]

        target = None
        if laws:
            # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë²•ë ¹ ìš°ì„ 
            exact_match = next((law for law in laws if law.get("ë²•ë ¹ëª…í•œê¸€") == law_name), None)
            if exact_match:
                target = exact_match
            else:
                # ì´ë¦„ì´ ê°€ì¥ ì§§ì€ ë²•ë ¹ ì„ íƒ (ì‹œí–‰ë ¹/ì‹œí–‰ê·œì¹™ ë°°ì œ ëª©ì )
                laws.sort(key=lambda x: len(x.get("ë²•ë ¹ëª…í•œê¸€", "")))
                target = laws[0]

        if target:
            raw_id = target.get("ë²•ë ¹ID")
            real_name = target.get("ë²•ë ¹ëª…í•œê¸€")
            return str(int(raw_id)) if raw_id and raw_id.isdigit() else raw_id, real_name
    except Exception as e:
        print(f"âš ï¸ ID ê²€ìƒ‰ ì‹¤íŒ¨ ({law_name}): {e}")
    return None, None


def get_parsed_articles(law_id, law_name):
    """ë²•ë ¹ ë³¸ë¬¸ XMLì„ ê°€ì ¸ì™€ ì¡°í•­ë³„ í…ìŠ¤íŠ¸ë¡œ íŒŒì‹±"""
    url = f"http://www.law.go.kr/DRF/lawService.do?OC={MOLEG_API_KEY}&target=eflaw&ID={law_id}&type=XML"
    parsed_docs = []
    try:
        response = requests.get(url, timeout=10)
        root = ET.fromstring(response.content)

        for unit in root.findall(".//ì¡°ë¬¸ë‹¨ìœ„"):
            if unit.find("ì¡°ë¬¸ì—¬ë¶€").text != "ì¡°ë¬¸":
                continue

            text_buffer = []
            for elem in unit.iter():
                if elem.text and elem.text.strip():
                    tag, text = elem.tag, elem.text.strip()
                    if tag == "ì¡°ë¬¸ë‚´ìš©":
                        text_buffer.append(text)
                    elif tag in ["í•­ë²ˆí˜¸", "í˜¸ë²ˆí˜¸", "ëª©ë²ˆí˜¸"]:
                        text_buffer.append(f"\n  {text}")

            full_text = "".join(text_buffer).strip()
            article_num = unit.find(".//ì¡°ë¬¸ë²ˆí˜¸")
            article_title = unit.find(".//ì¡°ë¬¸ëª…")

            metadata = {
                "source": law_name,
                "ì¡°ë¬¸ë²ˆí˜¸": article_num.text.strip() if article_num is not None and article_num.text else "N/A",
                "ì¡°ë¬¸ëª…": article_title.text.strip() if article_title is not None and article_title.text else "N/A",
            }

            if full_text:
                parsed_docs.append(Document(page_content=full_text, metadata=metadata))
    except Exception as e:
        print(f"âš ï¸ ë³¸ë¬¸ íŒŒì‹± ì‹¤íŒ¨ ({law_name}): {e}")
    return parsed_docs


# 3. ë©”ì¸ ì‹¤í–‰ ë¡œì§
def build_vector_db():
    print(f"ë²•ë ¹ ë°ì´í„° êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ì €ì¥ ê²½ë¡œ: {SAVE_PATH})")
    all_documents = []

    # 3-1. ë²•ë ¹ ë°ì´í„° ìˆ˜ì§‘
    for law_name in TARGET_LAWS:
        print(f"   ğŸ” '{law_name}' ê²€ìƒ‰ ì¤‘...")
        law_id, real_name = search_law_id(law_name)

        if law_id:
            print(f"   ğŸ“¥ '{real_name}'(ID:{law_id}) ë³¸ë¬¸ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±...")
            docs = get_parsed_articles(law_id, real_name)
            all_documents.extend(docs)
            print(f"      ğŸ‘‰ {len(docs)}ê°œ ì¡°í•­ ì¶”ì¶œ ì™„ë£Œ")
        else:
            print(f"      âŒ ë²•ë ¹ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ê²€ìƒ‰ì–´: {law_name})")

    if not all_documents:
        print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3-2. ë²¡í„°í™” ë° ì €ì¥
    print(f"âš¡ ì´ {len(all_documents)}ê°œ ì¡°í•­ ë²¡í„°í™” ì‹œì‘ (Model: jhgan/ko-sbert-nli)...")
    embeddings = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli", model_kwargs={'device': 'cpu'})
    vectorstore = FAISS.from_documents(all_documents, embeddings)

    os.makedirs(SAVE_PATH, exist_ok=True)
    vectorstore.save_local(SAVE_PATH)
    print(f"âœ… ì €ì¥ ì™„ë£Œ! DB ê²½ë¡œ: {os.path.abspath(SAVE_PATH)}")


if __name__ == "__main__":
    build_vector_db()
